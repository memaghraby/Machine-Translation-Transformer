{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import json\n",
    "from tensorflow import keras\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from nltk.translate import ibm1,AlignedSent,Alignment,PhraseTable,StackDecoder\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file, encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]  \n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    text_pairs.append((spa, eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spa_texts = [pair[0] for pair in train_pairs]\n",
    "train_eng_texts = [pair[1] for pair in train_pairs]\n",
    "tokenized_eng_sents = [word_tokenize(i) for i in train_eng_texts]\n",
    "tokenized_spa_sents = [word_tokenize(i) for i in train_spa_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_spa_texts[:10])\n",
    "print(\"===============\")\n",
    "print(tokenized_spa_sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_json = json.dumps(tokenized_spa_sents)\n",
    "f1 = open(\"spanish.json\",\"w\")\n",
    "f1.write(spanish_json)\n",
    "f1.close()\n",
    "english_json = json.dumps(tokenized_eng_sents)\n",
    "f2 = open(\"english.json\",\"w\")\n",
    "f2.write(english_json)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_model_generation():\n",
    "    bilingual_text = []\n",
    "    english_file = open(\"english.json\",\"r\")\n",
    "    english_text = english_file.read()\n",
    "    english_list = json.loads(english_text)\n",
    "    spanish_file = open(\"spanish.json\",\"r\")\n",
    "    spanish_text = spanish_file.read()\n",
    "    spanish_list = json.loads(spanish_text)\n",
    "    for iter in zip(english_list, spanish_list): #opposite\n",
    "        # Alignment must have mapping order\n",
    "        # FIXME SEARCH ON IMPLEMENTING THE ALIGNMENT CORRECTLY  \n",
    "        bilingual_text.append(AlignedSent(iter[0],iter[1]))\n",
    "    ibm1_model = ibm1.IBMModel1(bilingual_text,10)\n",
    "    return ibm1_model\n",
    "\n",
    "def language_model_generation():\n",
    "    # FIXME SPANISH.JSON OR SHOULD IT BE THE SAME ENLGISH.JSON\n",
    "    spanish_file = open(\"english.json\",\"r\") #opposite\n",
    "    spanish_text = spanish_file.read()\n",
    "    spanish_list = json.loads(spanish_text)\n",
    "    fdist = defaultdict(lambda: 1e-300)\n",
    "    # words = []\n",
    "    # for sentence in spanish_list:\n",
    "    #     for w in sentence:\n",
    "    #         words.append(w)\n",
    "    # fdist = nltk.FreqDist(words)\n",
    "    fdist = nltk.FreqDist(w for sentence in spanish_list for w in sentence)\n",
    "\n",
    "    #fdist.setdefault(lambda:1e-300, 1e-300)\n",
    "    language_model = type('',(object,),{'probability_change':lambda self,context,phrase:np.log(fdist[phrase]),'probability':lambda self,phrase:np.log(fdist[phrase])})()\n",
    "    return language_model\n",
    "\n",
    "def phrase_table_generation(ibm1_model):\n",
    "    phrase_table = PhraseTable()\n",
    "    for english_word in ibm1_model.translation_table.keys():\n",
    "        for spanish_word in ibm1_model.translation_table[english_word].keys():\n",
    "            phrase_table.add((spanish_word,),(english_word,), np.log(ibm1_model.translation_table[english_word][spanish_word]))\n",
    "    # for chinese_word in ibm1_model.translation_table.keys():\n",
    "    #     for english_word in ibm1_model.translation_table[chinese_word].keys():\n",
    "    #         phrase_table.add(chinese_word,english_word, np.log(ibm1_model.translation_table[chinese_word][english_word]))\n",
    "\n",
    "    return phrase_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model = translation_model_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_table = phrase_table_generation(translation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_table.translations_for(('Me',))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = language_model_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_decoder1 = StackDecoder(phrase_table,language_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stack_decoder1.translate(['Soy','bueno']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spa_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(5):\n",
    "    input_sentence = random.choice(test_spa_texts)\n",
    "    tokens = list(word_tokenize(input_sentence))\n",
    "    translated = stack_decoder1.translate(tokens)\n",
    "    print(input_sentence)\n",
    "    #print(tokens)\n",
    "    #print(translated)\n",
    "    print(' '.join(translated))\n",
    "    print(\"===================\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7e022b67859ae4a791bbcc1c75bde8b3a5bef3b9abb0060fdb4399d638fb2f6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
