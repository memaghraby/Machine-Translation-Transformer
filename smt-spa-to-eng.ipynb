{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import json\n",
    "from tensorflow import keras\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from nltk.translate import ibm1, AlignedSent, Alignment, PhraseTable, StackDecoder\n",
    "from collections import defaultdict\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file, encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    text_pairs.append((spa, eng))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spa_texts = [pair[0] for pair in train_pairs]\n",
    "train_eng_texts = [pair[1] for pair in train_pairs]\n",
    "tokenized_eng_sents = [word_tokenize(i) for i in train_eng_texts]\n",
    "tokenized_spa_sents = [word_tokenize(i) for i in train_spa_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ellos cierran la puerta a las cinco.', 'A Tom no le importa con quién salga Mary.', 'Él aceptó mi regalo.', 'No se permite fumar en esta sala.', '¿Cuánto me cobrará usted por el traje?', 'Es usted muy afortunado de tener tales amigos.', 'No soy bueno para hacer varias cosas a la vez.', 'No veo el coche de Tom.', 'Me levanté a las 7 de la mañana.', 'Tom quería la muerte de Mary porque ella había matado a sus hijos.']\n",
      "===============\n",
      "[['Ellos', 'cierran', 'la', 'puerta', 'a', 'las', 'cinco', '.'], ['A', 'Tom', 'no', 'le', 'importa', 'con', 'quién', 'salga', 'Mary', '.'], ['Él', 'aceptó', 'mi', 'regalo', '.'], ['No', 'se', 'permite', 'fumar', 'en', 'esta', 'sala', '.'], ['¿Cuánto', 'me', 'cobrará', 'usted', 'por', 'el', 'traje', '?'], ['Es', 'usted', 'muy', 'afortunado', 'de', 'tener', 'tales', 'amigos', '.'], ['No', 'soy', 'bueno', 'para', 'hacer', 'varias', 'cosas', 'a', 'la', 'vez', '.'], ['No', 'veo', 'el', 'coche', 'de', 'Tom', '.'], ['Me', 'levanté', 'a', 'las', '7', 'de', 'la', 'mañana', '.'], ['Tom', 'quería', 'la', 'muerte', 'de', 'Mary', 'porque', 'ella', 'había', 'matado', 'a', 'sus', 'hijos', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(train_spa_texts[:10])\n",
    "print(\"===============\")\n",
    "print(tokenized_spa_sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_json = json.dumps(tokenized_spa_sents)\n",
    "f1 = open(\"spanish.json\",\"w\")\n",
    "f1.write(spanish_json)\n",
    "f1.close()\n",
    "english_json = json.dumps(tokenized_eng_sents)\n",
    "f2 = open(\"english.json\",\"w\")\n",
    "f2.write(english_json)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Language Maps\n",
      "Loaded Spanish to English Matrix\n"
     ]
    }
   ],
   "source": [
    "from IBM_Model1.IBM_Model1 import IBM\n",
    "\n",
    "ibm_ourversion = IBM()\n",
    "ibm_ourversion.load()\n",
    "\n",
    "en_list = ibm_ourversion.getEnglishDict().keys()\n",
    "spa_list = ibm_ourversion.getSpanishDict().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_model_generation():\n",
    "    bilingual_text = []\n",
    "    english_file = open(\"english.json\",\"r\")\n",
    "    english_text = english_file.read()\n",
    "    english_list = json.loads(english_text)\n",
    "    spanish_file = open(\"spanish.json\",\"r\")\n",
    "    spanish_text = spanish_file.read()\n",
    "    spanish_list = json.loads(spanish_text)\n",
    "    for iter in zip(english_list, spanish_list): #opposite\n",
    "        bilingual_text.append(AlignedSent(iter[0],iter[1]))\n",
    "    ibm1_model = ibm1.IBMModel1(bilingual_text,10)\n",
    "    return ibm1_model\n",
    "\n",
    "def language_model_generation():\n",
    "    spanish_file = open(\"english.json\",\"r\") #opposite\n",
    "    spanish_text = spanish_file.read()\n",
    "    spanish_list = json.loads(spanish_text)\n",
    "    fdist = defaultdict(lambda: 1e-300)\n",
    "    # words = []\n",
    "    # for sentence in spanish_list:\n",
    "    #     for w in sentence:\n",
    "    #         words.append(w)\n",
    "    # fdist = nltk.FreqDist(words)\n",
    "    fdist = nltk.FreqDist(w for sentence in spanish_list for w in sentence)\n",
    "\n",
    "    #fdist.setdefault(lambda:1e-300, 1e-300)\n",
    "    language_model = type('',(object,),{'probability_change':lambda self,context,phrase:np.log(fdist[phrase]),'probability':lambda self,phrase:np.log(fdist[phrase])})()\n",
    "    return language_model\n",
    "\n",
    "def phrase_table_generation(ibm1_model):\n",
    "    phrase_table = PhraseTable()\n",
    "    for english_word in ibm1_model.translation_table.keys():\n",
    "        for spanish_word in ibm1_model.translation_table[english_word].keys():\n",
    "            phrase_table.add((spanish_word,),(english_word,), np.log(ibm1_model.translation_table[english_word][spanish_word]))\n",
    "    # for chinese_word in ibm1_model.translation_table.keys():\n",
    "    #     for english_word in ibm1_model.translation_table[chinese_word].keys():\n",
    "    #         phrase_table.add(chinese_word,english_word, np.log(ibm1_model.translation_table[chinese_word][english_word]))\n",
    "\n",
    "    return phrase_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-73c0f8740fd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslation_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslation_model_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-734eee752015>\u001b[0m in \u001b[0;36mtranslation_model_generation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglish_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspanish_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#opposite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mbilingual_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAlignedSent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mibm1_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mibm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIBMModel1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbilingual_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mibm1_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\translate\\ibm1.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentence_aligned_corpus, iterations, probability_tables)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_aligned_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_aligned_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\translate\\ibm1.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, parallel_corpus)\u001b[0m\n\u001b[0;32m    168\u001b[0m                     \u001b[0mnormalized_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                     \u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_given_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnormalized_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many_t_given_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnormalized_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;31m# M step: Update probabilities with maximum likelihood estimate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "translation_model = translation_model_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-735190a3ce21>:34: RuntimeWarning: divide by zero encountered in log\n",
      "  phrase_table.add((spanish_word,),(english_word,), np.log(ibm_ourversion.translate(english_word,spanish_word)))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-9df8ad62d816>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mphrase_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphrase_table_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-735190a3ce21>\u001b[0m in \u001b[0;36mphrase_table_generation\u001b[1;34m(ibm1_model)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0menglish_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0men_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mspanish_word\u001b[0m \u001b[1;32min\u001b[0m  \u001b[0mspa_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mphrase_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspanish_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglish_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mibm_ourversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglish_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspanish_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;31m# for chinese_word in ibm1_model.translation_table.keys():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m#     for english_word in ibm1_model.translation_table[chinese_word].keys():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\translate\\api.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, src_phrase, trg_phrase, log_prob)\u001b[0m\n\u001b[0;32m    329\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_phrases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_phrase\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_phrases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_phrase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_phrases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_phrase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_phrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\translate\\api.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m    329\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_phrases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_phrase\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_phrases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_phrase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_phrases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_phrase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_phrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "phrase_table = phrase_table_generation(translation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhraseTableEntry(trg_phrase=('I',), log_prob=-0.1707006859897912),\n",
       " PhraseTableEntry(trg_phrase=('me',), log_prob=-2.24894505153687)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_table.translations_for(('Me',))[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = language_model_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_decoder1 = StackDecoder(phrase_table,language_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d4dfebda2cc7>:27: RuntimeWarning: divide by zero encountered in log\n",
      "  language_model = type('',(object,),{'probability_change':lambda self,context,phrase:np.log(fdist[phrase]),'probability':lambda self,phrase:np.log(fdist[phrase])})()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'m\", 'good']\n"
     ]
    }
   ],
   "source": [
    "print(stack_decoder1.translate(['Yo', 'Soy','bueno']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d4dfebda2cc7>:27: RuntimeWarning: divide by zero encountered in log\n",
      "  language_model = type('',(object,),{'probability_change':lambda self,context,phrase:np.log(fdist[phrase]),'probability':lambda self,phrase:np.log(fdist[phrase])})()\n"
     ]
    }
   ],
   "source": [
    "test_spa_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(5):\n",
    "    input_sentence = random.choice(test_spa_texts)\n",
    "    tokens = list(word_tokenize(input_sentence))\n",
    "    translated = stack_decoder1.translate(tokens)\n",
    "    print(input_sentence)\n",
    "    #print(tokens)\n",
    "    #print(translated)\n",
    "    print(' '.join(translated))\n",
    "    print(\"===================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
