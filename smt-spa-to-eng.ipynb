{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\4Ever\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import json\n",
    "from tensorflow import keras\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from nltk.translate import ibm1, AlignedSent, Alignment, PhraseTable, StackDecoder\n",
    "from collections import defaultdict\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file, encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]  \n",
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    text_pairs.append((spa, eng))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spa_texts = [pair[0] for pair in train_pairs]\n",
    "train_eng_texts = [pair[1] for pair in train_pairs]\n",
    "tokenized_eng_sents = [word_tokenize(i) for i in train_eng_texts]\n",
    "tokenized_spa_sents = [word_tokenize(i) for i in train_spa_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_json = json.dumps(tokenized_spa_sents)\n",
    "f1 = open(\"spanish.json\",\"w\")\n",
    "f1.write(spanish_json)\n",
    "f1.close()\n",
    "english_json = json.dumps(tokenized_eng_sents)\n",
    "f2 = open(\"english.json\",\"w\")\n",
    "f2.write(english_json)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Henaaa\n",
      "Iteration :  0\n",
      "Finished first loop\n",
      "finish \n",
      "Iteration :  1\n",
      "Finished first loop\n",
      "finish \n",
      "Iteration :  2\n",
      "Finished first loop\n",
      "finish \n",
      "Iteration :  3\n",
      "Finished first loop\n",
      "finish \n",
      "Iteration :  4\n",
      "Finished first loop\n",
      "finish \n",
      "Iteration :  5\n",
      "Finished first loop\n",
      "finish \n",
      "Iteration :  6\n",
      "Finished first loop\n",
      "finish \n",
      "Iteration :  7\n",
      "Finished first loop\n",
      "finish \n",
      "Iteration :  8\n",
      "Finished first loop\n",
      "finish \n",
      "Iteration :  9\n",
      "Finished first loop\n",
      "finish \n",
      "Iteration :  10\n",
      "Finished first loop\n",
      "finish \n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "from IBM_Model1.IBM_Model1 import IBM\n",
    "\n",
    "ibm_ourversion = IBM(IBM.TRAINIG_MODE)\n",
    "# ibm_ourversion.load()\n",
    "ibm_ourversion.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def translation_model_generation(): #remove\n",
    "    bilingual_text = []\n",
    "    english_file = open(\"english.json\",\"r\")\n",
    "    english_text = english_file.read()\n",
    "    english_list = json.loads(english_text)\n",
    "    spanish_file = open(\"spanish.json\",\"r\")\n",
    "    spanish_text = spanish_file.read()\n",
    "    spanish_list = json.loads(spanish_text)\n",
    "    for iter in zip(english_list, spanish_list):\n",
    "        # Alignment must have mapping order\n",
    "        # FIXME SEARCH ON IMPLEMENTING THE ALIGNMENT CORRECTLY  \n",
    "        bilingual_text.append(AlignedSent(iter[0],iter[1]))\n",
    "    ibm1_model = ibm1.IBMModel1(bilingual_text, 10)\n",
    "    return ibm1_model\n",
    "\n",
    "def language_model_generation():\n",
    "    english_file = open(\"english.json\",\"r\")\n",
    "    english_text = english_file.read()\n",
    "    english_list = json.loads(english_text)\n",
    "\n",
    "    fdist = defaultdict(lambda: 1e-300)\n",
    "    #fdist = nltk.FreqDist(w for sentence in english_list for w in sentence)\n",
    "    for sentence in english_list:\n",
    "         for word in sentence:\n",
    "            fdist[word] += 1\n",
    "    language_model = type('', (object,),{'probability_change':lambda self,context,phrase:np.log(fdist[phrase]),'probability':lambda self,phrase:np.log(fdist[phrase])})()\n",
    "    return language_model\n",
    "\n",
    "\n",
    "def phrase_table_generation(ibm1_model):\n",
    "    phrase_table = PhraseTable()\n",
    "    translation_table = ibm_ourversion.getTranslationTable() #change to ibm1_model\n",
    "    for english_word in tqdm(translation_table):\n",
    "        for spanish_word in translation_table[english_word].keys():\n",
    "            phrase_table.add((spanish_word,),(english_word,), np.log(translation_table[english_word][spanish_word]))\n",
    "   \n",
    "    return phrase_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation_model = translation_model_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13622/13622 [04:43<00:00, 48.06it/s]\n"
     ]
    }
   ],
   "source": [
    "phrase_table = phrase_table_generation(ibm_ourversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = language_model_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stack_decoder1 = StackDecoder(phrase_table,language_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'im', 'good']\n",
      "['she', 'must', 'be', 'happy']\n",
      "['i', 'not', 'understand']\n",
      "['hello', 'me', 'call', 'john']\n",
      "['for', 'please', 'speaks', 'more', 'slowly']\n"
     ]
    }
   ],
   "source": [
    "print(stack_decoder1.translate([word.lower() for word in ['Yo', 'Soy','bueno']]) )\n",
    "print(stack_decoder1.translate([word.lower() for word in ['Ella', 'debe', 'ser', 'feliz']]))\n",
    "print(stack_decoder1.translate([word.lower() for word in ['Yo', 'no', 'comprendo']]))\n",
    "print(stack_decoder1.translate([word.lower() for word in ['Hola', 'me', 'llamo', 'Juan']]))\n",
    "print(stack_decoder1.translate([word.lower() for word in ['Por', 'favor', 'habla', 'más', 'despacio']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-623c823a19b5>:25: RuntimeWarning: divide by zero encountered in log\n",
      "  language_model = type('',(object,),{'probability_change':lambda self,context,phrase:np.log(fdist[phrase]),'probability':lambda self,phrase:np.log(fdist[phrase])})()\n",
      "c:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\translate\\stack_decoder.py:189: UserWarning: Unable to translate all words. The source sentence contains words not in the phrase table\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ella debe ser feliz.\n",
      "\n",
      "===================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-11d92e9193aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minput_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_spa_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtranslated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack_decoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#print(tokens)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\translate\\stack_decoder.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, src_sentence)\u001b[0m\n\u001b[0;32m    184\u001b[0m                         )\n\u001b[0;32m    185\u001b[0m                         \u001b[0mtotal_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_hypothesis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_translated_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0mstacks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_hypothesis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstacks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\translate\\stack_decoder.py\u001b[0m in \u001b[0;36mpush\u001b[1;34m(self, hypothesis)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \"\"\"\n\u001b[0;32m    480\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\translate\\stack_decoder.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(h)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \"\"\"\n\u001b[0;32m    480\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\translate\\stack_decoder.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuture_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m         \"\"\"\n\u001b[0;32m    392\u001b[0m         \u001b[0mOverall\u001b[0m \u001b[0mscore\u001b[0m \u001b[0mof\u001b[0m \u001b[0mhypothesis\u001b[0m \u001b[0mafter\u001b[0m \u001b[0maccounting\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlocal\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_spa_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(5):\n",
    "    input_sentence = random.choice(test_spa_texts)\n",
    "    tokens = list(word_tokenize(input_sentence))\n",
    "    translated = stack_decoder1.translate(tokens)\n",
    "    print(input_sentence)\n",
    "    #print(tokens)\n",
    "    #print(translated)\n",
    "    print(' '.join(translated))\n",
    "    print(\"===================\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "974da2f8eb465e3f44e385b0dbb2b4c0bbef747cf5af66c376c9b216143c0318"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
