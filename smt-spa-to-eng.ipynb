{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import json\n",
    "from tensorflow import keras\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from nltk.translate import ibm1, AlignedSent, Alignment, PhraseTable, StackDecoder\n",
    "from collections import defaultdict\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file, encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    text_pairs.append((spa, eng))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spa_texts = [pair[0] for pair in train_pairs]\n",
    "train_eng_texts = [pair[1] for pair in train_pairs]\n",
    "tokenized_eng_sents = [word_tokenize(i) for i in train_eng_texts]\n",
    "tokenized_spa_sents = [word_tokenize(i) for i in train_spa_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tom sacó la billetera y pagó la cuenta.', 'Pasaron dos semanas.', 'Tom abrió la cerveza.', 'No hay una sola alma en los estacionamientos.', '¿Cuánto tiempo trabajó Tom acá?', 'Tiende a enfadarse por nada.', 'Demasiados dulces engordan.', 'Algo que deberías saber acerca de mí es que soy un animal de costumbres.', '¿Lo tienes todo?', 'Pasé el fin de semana leyendo una larga novela.']\n",
      "===============\n",
      "[['Tom', 'sacó', 'la', 'billetera', 'y', 'pagó', 'la', 'cuenta', '.'], ['Pasaron', 'dos', 'semanas', '.'], ['Tom', 'abrió', 'la', 'cerveza', '.'], ['No', 'hay', 'una', 'sola', 'alma', 'en', 'los', 'estacionamientos', '.'], ['¿Cuánto', 'tiempo', 'trabajó', 'Tom', 'acá', '?'], ['Tiende', 'a', 'enfadarse', 'por', 'nada', '.'], ['Demasiados', 'dulces', 'engordan', '.'], ['Algo', 'que', 'deberías', 'saber', 'acerca', 'de', 'mí', 'es', 'que', 'soy', 'un', 'animal', 'de', 'costumbres', '.'], ['¿Lo', 'tienes', 'todo', '?'], ['Pasé', 'el', 'fin', 'de', 'semana', 'leyendo', 'una', 'larga', 'novela', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(train_spa_texts[:10])\n",
    "print(\"===============\")\n",
    "print(tokenized_spa_sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_json = json.dumps(tokenized_spa_sents)\n",
    "f1 = open(\"spanish.json\",\"w\")\n",
    "f1.write(spanish_json)\n",
    "f1.close()\n",
    "english_json = json.dumps(tokenized_eng_sents)\n",
    "f2 = open(\"english.json\",\"w\")\n",
    "f2.write(english_json)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Language Maps\n",
      "Loaded Spanish to English Matrix\n"
     ]
    }
   ],
   "source": [
    "from IBM_Model1.IBM_Model1 import IBM\n",
    "\n",
    "ibm_ourversion = IBM()\n",
    "ibm_ourversion.load()\n",
    "\n",
    "en_list = ibm_ourversion.getEnglishDict().keys()\n",
    "spa_list = ibm_ourversion.getSpanishDict().keys()\n",
    "# translationTable = ibm_ourversion.getTranslationTable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def translation_model_generation():\n",
    "    bilingual_text = []\n",
    "    english_file = open(\"english.json\",\"r\")\n",
    "    english_text = english_file.read()\n",
    "    english_list = json.loads(english_text)\n",
    "    spanish_file = open(\"spanish.json\",\"r\")\n",
    "    spanish_text = spanish_file.read()\n",
    "    spanish_list = json.loads(spanish_text)\n",
    "    for iter in zip(english_list, spanish_list): #opposite\n",
    "        bilingual_text.append(AlignedSent(iter[0],iter[1]))\n",
    "    ibm1_model = ibm1.IBMModel1(bilingual_text, 3)\n",
    "    return ibm1_model\n",
    "\n",
    "def language_model_generation():\n",
    "    spanish_file = open(\"english.json\",\"r\") #opposite\n",
    "    spanish_text = spanish_file.read()\n",
    "    spanish_list = json.loads(spanish_text)\n",
    "    fdist = defaultdict(lambda: 1e-300)\n",
    "    # words = []\n",
    "    # for sentence in spanish_list:\n",
    "    #     for w in sentence:\n",
    "    #         words.append(w)\n",
    "    # fdist = nltk.FreqDist(words)\n",
    "    fdist = nltk.FreqDist(w for sentence in spanish_list for w in sentence)\n",
    "\n",
    "    #fdist.setdefault(lambda:1e-300, 1e-300)\n",
    "    language_model = type('',(object,),{'probability_change':lambda self,context,phrase:np.log(fdist[phrase]),'probability':lambda self,phrase:np.log(fdist[phrase])})()\n",
    "    return language_model\n",
    "\n",
    "def phrase_table_generation(ibm1_model):\n",
    "    phrase_table = PhraseTable()\n",
    "    for english_word in en_list:\n",
    "        for spanish_word in spa_list:\n",
    "            # phrase_table.add((spanish_word,),(english_word,), np.log(ibm1_model.translation_table[english_word][spanish_word]))\n",
    "            print(ibm1_model.translation_table[english_word][spanish_word], ibm_ourversion.translate(english_word, spanish_word))\n",
    "    # # for chinese_word in ibm1_model.translation_table.keys():\n",
    "    # for english_word in tqdm(en_list):\n",
    "    #     for spanish_word in spa_list:\n",
    "    #         phrase_table.add((spanish_word,),(english_word,), np.log(ibm_ourversion.translate(english_word,spanish_word)))\n",
    "    \n",
    "\n",
    "    return phrase_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model = translation_model_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f6f508dd39f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mphrase_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphrase_table_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslation_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-d59d6a9e9a49>\u001b[0m in \u001b[0;36mphrase_table_generation\u001b[1;34m(ibm1_model)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mspanish_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mibm1_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslation_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menglish_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m# phrase_table.add((spanish_word,),(english_word,), np.log(ibm1_model.translation_table[english_word][spanish_word]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mibm1_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslation_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menglish_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mspanish_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mibm_ourversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglish_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspanish_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;31m# # for chinese_word in ibm1_model.translation_table.keys():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# for english_word in tqdm(en_list):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\University\\GP\\Machine-Translation-Transformer\\IBM_Model1\\IBM_Model1.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, en_word, spa_word)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspa_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[0midx_spa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspa_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mspa_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m     \u001b[0midx_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0men_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_spa_en_mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_en\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_spa\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "phrase_table = phrase_table_generation(translation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhraseTableEntry(trg_phrase=('I',), log_prob=-0.1707006859897912),\n",
       " PhraseTableEntry(trg_phrase=('me',), log_prob=-2.24894505153687)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_table.translations_for(('Me',))[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = language_model_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_decoder1 = StackDecoder(phrase_table,language_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d4dfebda2cc7>:27: RuntimeWarning: divide by zero encountered in log\n",
      "  language_model = type('',(object,),{'probability_change':lambda self,context,phrase:np.log(fdist[phrase]),'probability':lambda self,phrase:np.log(fdist[phrase])})()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'m\", 'good']\n"
     ]
    }
   ],
   "source": [
    "print(stack_decoder1.translate(['Yo', 'Soy','bueno']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d4dfebda2cc7>:27: RuntimeWarning: divide by zero encountered in log\n",
      "  language_model = type('',(object,),{'probability_change':lambda self,context,phrase:np.log(fdist[phrase]),'probability':lambda self,phrase:np.log(fdist[phrase])})()\n"
     ]
    }
   ],
   "source": [
    "test_spa_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(5):\n",
    "    input_sentence = random.choice(test_spa_texts)\n",
    "    tokens = list(word_tokenize(input_sentence))\n",
    "    translated = stack_decoder1.translate(tokens)\n",
    "    print(input_sentence)\n",
    "    #print(tokens)\n",
    "    #print(translated)\n",
    "    print(' '.join(translated))\n",
    "    print(\"===================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
