{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "#This block is just used to get test sentences to translate and should be removed\n",
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n",
    "\n",
    "with open(text_file, encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    eng = \"[start] \" + eng + \" [end]\"\n",
    "    text_pairs.append((spa, eng))\n",
    "\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_strip_chars = string.punctuation + \"¿\"\n",
    "eng_strip_chars = string.punctuation.replace(\"[\", \"\")\n",
    "eng_strip_chars = eng_strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 25\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def eng_custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(eng_strip_chars), \"\")\n",
    "\n",
    "def spa_custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(spa_strip_chars), \"\")\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=eng_custom_standardization,\n",
    ")\n",
    "spa_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=spa_custom_standardization,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading transformer and vectorization of both languages\n",
    "from_disk = pickle.load(open(\"spa_vectorization.pkl\", \"rb\"))\n",
    "spa_vectorization = TextVectorization.from_config(from_disk['config'])\n",
    "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
    "spa_vectorization.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "spa_vectorization.set_weights(from_disk['weights'])\n",
    "\n",
    "\n",
    "from_disk = pickle.load(open(\"eng_vectorization.pkl\", \"rb\"))\n",
    "eng_vectorization = TextVectorization.from_config(from_disk['config'])\n",
    "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
    "eng_vectorization.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "eng_vectorization.set_weights(from_disk['weights'])\n",
    "\n",
    "\n",
    "transformer = tf.saved_model.load('translator-transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Esa es mi escuela.', 'No estaba borracho.', 'El anciano era querido por todos.', 'Quiero una respuesta a esa pregunta.', 'Pensé que tal vez quisieran saber.', 'En el alfabeto, la B va después de la A.', 'Cerrá los ojos por tres minutos.', 'Para mi sorpresa, ella no pudo contestar a la pregunta.', 'Ella participó en el concurso.', 'Me costó un rato largo el asimilar lo que ella estaba diciendo.']\n",
      "Él es muy guapo.\n",
      "[start] he is very goodlooking [end]\n",
      "===================\n",
      "Extraño tanto París.\n",
      "[start] i miss paris so much [end]\n",
      "===================\n",
      "Sin su ayuda, yo estaría muerto.\n",
      "[start] without her help i would be dead [end]\n",
      "===================\n",
      "Tom no parece querer nuestra ayuda, ¿verdad?\n",
      "[start] tom doesnt seem to want our help [end]\n",
      "===================\n",
      "Quiero ver a tu hermana mayor.\n",
      "[start] i want to see your older sister [end]\n",
      "===================\n",
      "Me lo encontré de casualidad por la calle.\n",
      "[start] i met him on the street [end]\n",
      "===================\n",
      "Tom es el amigo de Mary.\n",
      "[start] tom is marys friend [end]\n",
      "===================\n",
      "El detective se disfrazó de viejo caballero.\n",
      "[start] the the [end]\n",
      "===================\n",
      "Tom se puso a cubierto.\n",
      "[start] tom got it to be covered [end]\n",
      "===================\n",
      "Este tío es cojonudo.\n",
      "[start] this uncle is a the the the the uncle [end]\n",
      "===================\n",
      "Estoy satisfecho con mi salario actual.\n",
      "[start] im satisfied with my present [end]\n",
      "===================\n",
      "Dígame el camino a la oficina de correos, por favor.\n",
      "[start] please please tell me the way to the jam [end]\n",
      "===================\n",
      "¿Y si me equivoco?\n",
      "[start] what if im wrong [end]\n",
      "===================\n",
      "Si él no viene, nosotros no iremos.\n",
      "[start] if he doesnt come we wont come [end]\n",
      "===================\n",
      "Todavía no ha sonado la campana.\n",
      "[start] the [end]\n",
      "===================\n",
      "Eso lo va a hacer muy feliz a Tom.\n",
      "[start] thats going to make tom very happy [end]\n",
      "===================\n",
      "Él tomó el bus equivocado por error.\n",
      "[start] he took the wrong bus for mistake [end]\n",
      "===================\n",
      "Me divertí mucho.\n",
      "[start] i had a lot of fun [end]\n",
      "===================\n",
      "Tom tiene que hablar en francés en el trabajo.\n",
      "[start] tom has to speak french at work [end]\n",
      "===================\n",
      "Por este camino se llega al museo.\n",
      "[start] this road will take you to the museum [end]\n",
      "===================\n",
      "Él vive cerca de mi casa.\n",
      "[start] he lives near my house [end]\n",
      "===================\n",
      "Tom está respirando.\n",
      "[start] tom is on the little he is [end]\n",
      "===================\n",
      "Claro que recuerdo el incidente bastante bien.\n",
      "[start] i do remember the incident quite well [end]\n",
      "===================\n",
      "No se puede huir de la edad.\n",
      "[start] it is not old enough to escape [end]\n",
      "===================\n",
      "Ellos estaban listos para la acción.\n",
      "[start] they were ready to the i have to get it [end]\n",
      "===================\n",
      "Es tan fácil.\n",
      "[start] its easy [end]\n",
      "===================\n",
      "La gente está enfadada.\n",
      "[start] the people are angry [end]\n",
      "===================\n",
      "Siempre estás viendo la televisión.\n",
      "[start] you are always watching tv [end]\n",
      "===================\n",
      "Tom trató de levantarse, pero no pudo.\n",
      "[start] tom tried to get up but couldnt [end]\n",
      "===================\n",
      "Solo somos amigos.\n",
      "[start] were only friends [end]\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "eng_vocab = eng_vectorization.get_vocabulary()\n",
    "eng_index_lookup = dict(zip(range(len(eng_vocab)), eng_vocab))\n",
    "max_decoded_sentence_length = 25\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = spa_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = eng_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = eng_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_spa_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_spa_texts)\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print(\"===================\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e284ee3255a07ad8bf76694974743c4c81cb57e7c969474d752d949b11d721e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
